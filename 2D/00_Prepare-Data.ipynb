{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Medical Decathlon dataset\n",
    "\n",
    "The [Medical Segmentation Decathlon (MSD)](http://medicaldecathlon.com/) is a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. This allows: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are we converting the 3D MRI scans to 2D slices?\n",
    "\n",
    "In this part of the tutorial, you will convert the Medical Decathlon dataset from [Nifti](https://nifti.nimh.nih.gov/) into NumPy format. Note that in the [3D model directory](../3D), we simply use the 3D Nifti files in our data loader. A 3D model is certainly the preferred method when using 3D datasets and will lead to higher accuracy models in general. However, for this demo we want to work with a simple 2D model and so it is faster to first extract the 2D slices into separate NumPy files and load the data as 2D. A batch in the 2D case refers to several 2D slices (probably from different 3D scans).\n",
    "\n",
    "To begin, you will to download the raw dataset from the Medical Decathlon website (http://medicaldecathlon.com), extract the data (untar), and follow the instructions in this notebook.\n",
    "\n",
    "You may wish to use the code snippet below to easily download the dataset (uncomment to execute)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "wget https://raw.githubusercontent.com/pavanjadhaw/gdown.pl/master/gdown.pl && chmod u+x gdown.pl\n",
    "./gdown.pl https://drive.google.com/file/d/1A2IU8Sgea1h3fYLpYtFb2v7NYdMjvEhU/view?usp=sharing Task01_BrainTumour.tar\n",
    "mkdir data\n",
    "mkdir data/decathlon\n",
    "tar -xf Task01_BrainTumour.tar --directory ./data/decathlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"../data/decathlon/Task01_BrainTumour\"  # directory for the original data\n",
    "save_path = \"../data/decathlon/Task01_BrainTumour/2D_model\"   # directory to save NumPy files\n",
    "seed = 816                                         # Random seed\n",
    "train_test_split = 0.85                            # Train/test split value (Percentage of dataset to keep for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BraTS Dataset\n",
    "\n",
    "The [Brain Tumor Segmentation Dataset](https://www.med.upenn.edu/sbia/brats2018/data.html) contains multi-institutional pre-operative 3D MRI scans and focuses on the segmentation of intrinsically heterogeneous (in appearance, shape, and histology) brain tumors, namely gliomas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the training file names from the data directory.\n",
    "Decathlon should always have a dataset.json file in the\n",
    "subdirectory which lists the experiment information including\n",
    "the input and label filenames.\n",
    "\"\"\"\n",
    "\n",
    "json_filename = os.path.join(data_path, \"dataset.json\")\n",
    "\n",
    "try:\n",
    "    with open(json_filename, \"r\") as fp:\n",
    "        experiment_data = json.load(fp)\n",
    "        \n",
    "    # Print information about the Decathlon experiment data\n",
    "    print(\"*\" * 30)\n",
    "    print(\"=\" * 30)\n",
    "    print(\"Dataset name:        \", experiment_data[\"name\"])\n",
    "    print(\"Dataset description: \", experiment_data[\"description\"])\n",
    "    print(\"Tensor image size:   \", experiment_data[\"tensorImageSize\"])\n",
    "    print(\"Dataset release:     \", experiment_data[\"release\"])\n",
    "    print(\"Dataset reference:   \", experiment_data[\"reference\"])\n",
    "    print(\"Dataset license:     \", experiment_data[\"licence\"])  # sic\n",
    "    print(\"Modality:            \", experiment_data[\"modality\"]) \n",
    "    print(\"Labels:              \", experiment_data[\"labels\"]) \n",
    "    print(\"Training set size :  \", experiment_data[\"numTraining\"]) \n",
    "    print(\"=\" * 30)\n",
    "    print(\"*\" * 30)\n",
    "except IOError as e:\n",
    "    print(\"File {} doesn't exist. \\nIt should be part of the \"\n",
    "          \"Decathlon directory.\\nDid you download and \"\n",
    "          \"extract Task01_BrainTumour.tar to directory {}?\".format(json_filename, data_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Testing Splits\n",
    "\n",
    "Although the MSD has separate training, validation, and testing splits, the testing directory does not contain the ground truth annotations. Instead, we'll split the training dataset into new training, validation, and testing datasets so that we'll have ground truth annotations for all 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Randomize the file list. Then separate into training and\n",
    "validation lists. We won't use the testing set since we\n",
    "don't have ground truth masks for this; instead we'll\n",
    "split the validation set into separate test and validation\n",
    "sets.\n",
    "\"\"\"\n",
    "# Set the random seed so that always get same random mix\n",
    "np.random.seed(seed)\n",
    "numFiles = experiment_data[\"numTraining\"]\n",
    "idxList = np.arange(numFiles)  # List of file indices\n",
    "randomList = np.random.random(numFiles)  # List of random numbers\n",
    "\n",
    "# Random number go from 0 to 1. So anything above\n",
    "# args.train_split is in the validation list.\n",
    "trainList = idxList[randomList < train_test_split]\n",
    "\n",
    "# Now we'll just split the remaining files into 50% validation and 50% testing datasets\n",
    "otherList = idxList[randomList >= train_test_split]\n",
    "randomList = np.random.random(len(otherList))  # List of random numbers\n",
    "validateList = otherList[randomList >= 0.5]\n",
    "testList = otherList[randomList < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_raw_to_npy import convert_raw_data_to_numpy\n",
    "\n",
    "convert_raw_data_to_numpy(trainList, validateList, testList,\n",
    "                          data_path,\n",
    "                          experiment_data,\n",
    "                          save_path, resize=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Segmentation Decathlon (MSD)\n",
    "\n",
    "The raw dataset has the CC-BY-SA 4.0 license. https://creativecommons.org/licenses/by-sa/4.0/. \n",
    "A paper describing the MSD is available [here](https://arxiv.org/abs/1902.09063).\n",
    "\n",
    "### References for the BraTS Dataset\n",
    "\n",
    "[1] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. \"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694\n",
    "\n",
    "[2] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., \"Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features\", Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117\n",
    "\n",
    "[3] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, et al., \"Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge\", arXiv preprint arXiv:1811.02629 (2018)\n",
    "\n",
    "In addition, if there are no restrictions imposed from the journal/conference you submit your paper about citing \"Data Citations\", please be specific and also cite the following:\n",
    "\n",
    "[4] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., \"Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection\", The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q\n",
    "\n",
    "[5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., \"Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-LGG collection\", The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.GJQ7R0EF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) 2019-2020 Intel Corporation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
